
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am doctoral student at the Institute of Applied Analysis at Ulm University, expected to graduate in early 2024. My research in the group of Prof. Dr. Emil Wiedemann focuses on statistical concepts in mathematical fluid dynamics, motivated by statistical descriptions of turbulence.\nNext to a general interest in mathematical modeling, I am also interested in machine learning. The latter subject is not only fascinating for its widespread applications but also for its theory being a combination of various intriguing mathematical subjects.\nNews: I submitted my doctoral thesis on 10th of October 2023. The defense is likely going to be in January 2024. ","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am doctoral student at the Institute of Applied Analysis at Ulm University, expected to graduate in early 2024. My research in the group of Prof. Dr. Emil Wiedemann focuses on statistical concepts in mathematical fluid dynamics, motivated by statistical descriptions of turbulence.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"From my third semester as a student at Ulm University on, I was tutoring and grading undergraduate students in math and computer science programmes. The classes mostly involved the fundamentals of mathematics such as analysis and linear algebra.\nMy stay at Syracuse University as a graduate student was partially funded by a teaching assistantship, where I was giving three to four recitations each week for calculus classes and helped students at the weekly calculus help desk.\nIn my position as a doctoral candidate and scientific assistant at Ulm University, my duties involve creating weekly assignment sheets as well as the final exams, administering the grading thereof and presenting the solutions in an auditorium.\nThe corresponding classes were mostly on the fundamentals of mathematics. Some more advanced courses for which I organized the exercises include the subjects of\nHyperbolic conservation laws (taught by Prof. Dr. Emil Wiedemann) Elements of the calculus of variations (taught by Dr. Nicola Zamponi) Functional analysis (for data science) (taught by Prof. Dr. Anna Dall’Acqua) For a complete overview of the classes I taught in the past years, see my university profile.\nDuring the COVID-19 pandemic, I recorded my exercise sessions. If you would like to get a first impression of me teaching in the classroom, have a look at an excerpt of one the sessions I recorded back then (Analysis 1, winter semester 2020/2021, in German).\nProblem: Zeigen Sie die folgende Aussage mittels vollständiger Induktion.\nFür jede natürliche Zahl $n\\in\\mathbb{N}$ ergibt die Summe der Quadrate der ersten $n$ natürlichen Zahlen $\\frac{1}{6}n(n+1)(2n+1)$.\n","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"634b6f0e52b8016c3050abf45ec6b0f4","permalink":"https://raphael-wagner.netlify.app/academia/teaching/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/academia/teaching/","section":"academia","summary":"An overview of my teaching experience during the past years.","tags":["Teaching"],"title":"Teaching","type":"academia"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\rFigure 1: A fancy pie chart.\r","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"bf1eb249db79f10ace7d22321494165a","permalink":"https://raphael-wagner.netlify.app/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":[""],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://raphael-wagner.netlify.app/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://raphael-wagner.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://raphael-wagner.netlify.app/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://raphael-wagner.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"I first came across the subject of machine learning in a graduate level course on pattern recognition as part of my minor in computer science.\nThe area focuses on the development of processes and algorithms which enable computers to learn a task from experience, driven by some performance measure. In particular, the way a task will be performed by a trained machine learning algorithm is not hard coded. In fact, for complex models such as deep neural networks, it appears to be difficult to extract patterns which the model has learned and by which it operates.\nBy now, most academics I believe have heard or come across this subject of machine learning due to the broad spectrum of tasks for which there are machine learning methods available (supervised problems: classification, regression, … and unsupervised problems: clustering and anomaly detection). Not to mention its vast success in different areas, e.g., detecting credit card fraud in bank statements or brain tumors in scans.\nEven in the numerical analysis of partial differential equations, machine learning algorithms are starting to establish themselves. In particular, in the earth sciences, where also a lot of data is collected, the simulation of models nowadays, by my understanding, is not only based on the numerical computation of the involved equations from physics, but is usually combined with data driven approaches. I was lucky to be able to attend talks at the mathematical colloquium at Ulm University given by Prof. Dr. Jakob Runge and Prof. Dr. Gitta Kutyniok who are experts in the area.\nNowadays, there is a ton of books and other resources available to teach machine learning methods to everyone with a very basic understanding of statistics and programming. Recently, I have been going through Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, which I found to be very accessible.\nBut even casting aside all the applications, many methods have a rather deep and interesting underlying mathematical foundation. The example that I recently came across is that of reproducing kernel Hilbert spaces (RKHS), which I thought I could shed some light on here. The nice part is that the theory of RKHS not only explains why methods such as ridge regression work and are reasonable, the theory also explains why the user does not need to understand its rather deep underlying theory to apply it, as it works in the background.\nRKHS play a role for certain regression and classification methods. Suppose we are given data which lies in some set $X$. Here, $X$ could be a set of text files, numerical data, images, etc. or combinations thereof. We suppose that each element in $X$ has a true class (classification), for instance when $X$ is the set of all dog or cat pictures and we have two classes, cats and dogs, or a true or accurate value, such as housing prizes, where $X$ could be the set of data of houses in an area. These classes or accurate values are usually called labels.\nSuppose that we have input data $x_1, … , x_n \\in X$ for $n \\in \\mathbb{N}$ properties and we know the associated labels $y_1,…,y_n \\in \\mathbb{R}$. Then $(x_1,y_1),…,(x_n,y_n)$ is our training data.\nWe now wish to be able to be able to predict the label for any given input data $x \\in X$. We express this by a function $f\\colon X \\to \\mathbb{R}$, where we would hope that by choosing $f$ in a way that it is accurate on the training set, it will also be accurate on new data (the problem of overfitting will be a addressed in a moment).\nWe measure accuracy by choosing a performance measure $V\\colon X \\times \\mathbb{R} \\to [0,\\infty)$, where $V(f(x_i),y_i)$ should be small if $f(x_i) \\sim y_i$ and large if $f(x_i)$ and $y_i$ deviate largely. A standard example for a performance measure is the square distance $$ V(f(x_i),y_i) = |f(x_i) - y_i|^2.$$ Taking the mean over the training set, we obtain a loss-function for our regression function $f\\colon X \\to \\mathbb{R}$: $$ L(f) = \\frac{1}{n}\\sum_{i=1}^n V(f(x_i),y_i). $$ There are many valid choices for performance measures and loss functions. The main property that they usually all have in common is (strict) convexity. Under further assumptions, this leads mathematically to the existence of a unique minimum of the function and, at least in theory, to convergence of approximative schemes to this minimum.\nUsually, by a fundamental study of the available training data, one has an a priori idea on how simple or complex the relationship between the input data $x \\in X$ and the labels $y \\in \\mathbb{R}$ is and makes an a piori choice for one or several general models, e.g., linear or polynomials models, neural networks and so on. Therefore, instead of considering all functions $f\\colon X \\to \\mathbb{R}$ as possible regression functions, we restrict ourselves to a subset of functions $H \\subset \\lbrace f\\colon X \\to \\mathbb{R} \\rbrace.$ The regression problem now lies in finding $$ f^* := \\underset{f \\in H}{\\operatorname{argmin}} …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"32c042111ded50ca96261c8cd438caec","permalink":"https://raphael-wagner.netlify.app/academia/ml/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/academia/ml/","section":"academia","summary":"A brief outline of some aspects of mathematical machine learning that interest me. As an example, I describe some more details for the subject of reproducing kernel Hilbert spaces and ridge regression.","tags":["Machine learning"],"title":"Machine learning","type":"academia"},{"authors":null,"categories":null,"content":"I first came across the subject of machine learning in a graduate level course on pattern recognition as part of my minor in computer science. The area focuses on the development of processes and algorithms which enable computers to learn a task from experience measured by some performance measure. In particular, the way a task will be performed by a trained machine learning algorithm is not hard coded. In fact, for complex models such as deep neural networks, it appears to be difficult to extract rules by which the model has learned and by which it operates.\nBy now, most academics I believe have heard or come across this subject of machine learning due to the broad spectrum of tasks for which there are machine learning methods available (supervised problems: classification, regression, … and unsupervised problems: clustering and anomaly detection) and its vast success in different areas, e.g., detecting credit card fraud in bank statements or brain tumors in scans. Even in the numerical analysis of partial differential equations, machine learning algorithms are establishing themselves. In particular, in the earth sciences, where also a lot of data is collected, the simulation of models nowadays, by my understanding, is not only based on the numerical computation of the involved equations from physics, but is usually combined with data driven approaches. I was lucky to be able to attend talks at the mathematical colloquium at Ulm University given by Prof. Dr. Jakob Runge and Prof. Dr. Gitta Kutyniok who are experts in the area.\nNowadays, there is a ton of books and other resources available to teach machine learning methods to everyone with a very basic understanding of statistics and basic programming skills. Recently, I have been going through Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, which I found to be very accessible.\nBut even casting aside all the applications, many methods have a rather deep underlying mathematical foundation. The example that I recently came across is that of reproducing kernel Hilbert spaces (RKHS), which I though I could shed some light on here. The nice part is that the theory of reproducing kernel Hilbert spaces not only explains why methods such as ridge regression work and are reasonable, the theory also explains why the user does not need to understand it, as it works in the background.\nReproducing kernel Hilbert spaces play a role for regression and classification problems. Suppose we are given data which lies in some set $X$. Here, $X$ could be a set of text files, numerical data, images, etc. or combinations thereof. We suppose that each element in $X$ has a true class (classification), for instance when $X$ is the set of all dog or cat pictures and we have two classes, cat or dog, or a true or accurate value, such housing prize, where $X$ could be the set of data of housing data in an area. Let us go with the latter example. Suppose that we have data $x_1, … , x_n \\in X$ for $n \\in \\mathbb{N}$ properties and we know the associated housing prizes $y_1,…,y_n \\in \\mathbb{R}$ for which property (for instance estimated by experts). Then $(x_1,y_1),…,(x_n,y_n)$ is our training data.\nWe now wish to be able to be able to predict an accurate housing price for any given housing data $x \\in X$. We express this by a function $f\\colon X \\to \\mathbb{R}$. We would hope that by choosing $f$ in a way that it is accurate on the training set, it will also accurate on new data. We measure accuracy by choosing a performance measure $V\\colon X \\times \\mathbb{R} \\to [0,\\infty)$, where $V(f(x_i),y_i)$ should be small if $f(x_i) \\sim y_i$ and large if $f(x_i)$ and $y_i$ deviate largely. A standard example for a performance measure is the square distance $$ V(f(x_i),y_i) = |f(x_i) - y_i|^2.$$ Taking the mean over the training set, we obtain a loss-function for our regression function $f\\colon X \\to \\mathbb{R}$. $$ L(f) = \\frac{1}{n}\\sum_{i=1}^n V(f(x_i),y_i). $$ There are many valid choices for performance measures and loss functions. The main property that they usually all have in common is (strict) convexity, which (under further assumptions) leads mathematically to the existence of a unique minimum of the function and at least in theory, to convergence of approximative schemes to this minimum.\nUsually, one has an a priori idea on how simple or complex the relationship between the data $x \\in X$ and the labels $y \\in \\mathbb{R}$ is and makes and a piori choice for one or several general models, e.g., linear or polynomials models. Therefore, instead of considering all functions $f\\colon X \\to \\mathbb{R}$ as possible regression functions, we restrict ourselves to a subset of functions $H \\subset \\lbrace f\\colon X \\to \\mathbb{R} \\rbrace.$ The regression problem now lies in finding $$ f^* := \\underset{f \\in H}{\\operatorname{argmin}} L(f). $$\nMinimization problems tend to be very convenient to handle in the setting of Hilbert spaces. Why? Hilbert spaces are …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"bf5c9dddb4c5894761b8c93aad6a9923","permalink":"https://raphael-wagner.netlify.app/other/ml/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/other/ml/","section":"other","summary":"A brief outline of some aspects of mathematical machine learning that interest me. I describe some more details for the subject of reproducing kernel Hilbert spaces and ridge regression.","tags":["ML"],"title":"Mathematics of machine learning","type":"other"},{"authors":null,"categories":null,"content":"In my research, I study statistical notions of solutions for certain equations in mathematical fluids mechanics. While the development of this theory tends to be quite abstract, some motivation comes from studying and experimenting with turbulent flows. Indeed, it is a longstanding and widely accepted view in turbulence theory that turbulent flows are more accurately described by probabilistic and statistical rather than purely deterministic models. Indeed, one of the fundamental aspects of a turbulent flow is the difficulty in the precise prediction of its behavior.\nTherefore, in the applied sciences, models of turbulent flows typically have a stochastic component and consider averages of the quantities in the governing mathematical equations.\nBelow you can see two signals using the same parameters, produced by a Matlab implementation by E. Cheynet1 of the von Kármán wind turbulence model, which is based on the Reynolds-averaged Navier-Stokes (RANS) equations. This model has proven to provide a fairly accurate basis for the description of wind turbulence.\nIf we consider the final histograms below, we note that both look somewhat similar. Of course, our sample is very small and we only repeated the simulation twice. More importantly, I have to admit that the model and its simulation already includes some randomness, whereby this result is not very surprising. However, this behavior is also observed in real measurement data from wind tunnels, see for instance U. Frisch2.\nAlternative and probabilistic approaches to the models and equations from fluid mechanics also stem from the fact that existence of a unique, physically relevant solution for these equations is often unknown. In fact, one of the 1 Mil. $ Millenium prize problems focuses around this issue.\nIn the following section, I will describe a little bit more in depth what my research area is about and what problems I considered in my articles and preprints.\nIn my research, I primarily consider the incompressible Euler $$ \\begin{equation} \\begin{split} \\partial_t u + (u\\cdot\\nabla)u + \\nabla p + \\gamma u \u0026amp;= f,\\ \\operatorname{div} u \u0026amp;= 0, \\end{split} \\end{equation} $$ and Navier-Stokes equations $$ \\begin{equation} \\begin{split} \\partial_t u + (u\\cdot\\nabla)u + \\nabla p + \\gamma u - \\nu\\Delta u \u0026amp;= f,\\ \\operatorname{div} u \u0026amp;= 0, \\end{split} \\end{equation} $$ formulated here in velocity $u \\colon (0,T) \\times \\Omega \\to \\mathbb{R}^d$ with pressure $p\\colon (0,T) \\times \\Omega \\to \\mathbb{R}$, an external force $f\\colon (0,T) \\times \\Omega \\to \\mathbb{R}^d$, kinematic viscosity $\\nu \u0026gt; 0$ and Ekman damping constant $\\gamma \\geq 0$ on a domain $\\Omega \\subset \\mathbb{R}^d$ until some time $T \u0026gt; 0$. Both equations are fundamental equations in fluid mechanics, yet mathematically, many standard questions in the theory of partial differential equations such as existence, uniqueness, and regularity are still open. Let me first explain what these properties mean and then point out a small selection of the currently open issues related to these.\nExistence: For any initial state of the fluid and other fixed parameters such as boundary data, we would like there to exist a solution to these fluid equations which describes the fluid at any later time. After all, we describe a physical problem and would expect such solutions to exist.\nExistence of (weak) solutions for the 3D Navier-Stokes equations has been known since the work of Leray in 1934. For the 2D Euler equations, many general existence results for weak solutions have been obtained in the past 15 years using the convex integration machinery by De Lellis and Székelyhidi Jr.\nExistence of physically relevant solutions on any time interval, however, is generally still unknown. I will further elaborate on this in the explanation on regularity. Uniqueness: Ideally, such a solution should also be unique. After all, if all the data is fixed, we would intuitively expect that the behavior of the fluid is deterministic, rather than having to deal with the possibility of the fluid behaving (at random) in different ways.\nJust in the past year, however, Albritton, Brué and Colombo proved that for a certain external force $f$, the Leray-solutions are in fact non-unique. Moreover, the convex integration solutions for the 2D Euler equations are vastly non-unique, in fact, this technique yields infinitely many solutions for any given initial state. To resolve this issue, it is an ongoing task to find further conditions which single out physical solutions. Regularity: This means that in a certain way, if the data has nice properties which correspond to real physical fluids, then the solutions at least maintains that regularity.\nIn contrast to the previously mentioned weak solutions, regular solutions are unique. However, it is generally unknown if regular initial data yields regular solutions globally in time for the 3D Navier-Stokes equations. If this was false, it would mean that a nice initial state of the fluid could in time …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9596d06faade535e41bc713c808f1cf5","permalink":"https://raphael-wagner.netlify.app/academia/research/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/academia/research/","section":"academia","summary":"A brief description of my research in mathematical fluid mechanics, conducted at the Institute of Applied Analysis at Ulm University during my time as a doctoral candidate.","tags":["Research"],"title":"Research","type":"academia"},{"authors":["","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://raphael-wagner.netlify.app/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://raphael-wagner.netlify.app/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]