<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academia | Raphael Wagner</title>
    <link>https://raphael-wagner.netlify.app/de/academia/</link>
      <atom:link href="https://raphael-wagner.netlify.app/de/academia/index.xml" rel="self" type="application/rss+xml" />
    <description>Academia</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>de</language><lastBuildDate>Wed, 01 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://raphael-wagner.netlify.app/media/icon_hu054d31ac38a623158e2d971b85a39826_2241_512x512_fill_lanczos_center_3.png</url>
      <title>Academia</title>
      <link>https://raphael-wagner.netlify.app/de/academia/</link>
    </image>
    
    <item>
      <title>Lehre</title>
      <link>https://raphael-wagner.netlify.app/de/academia/teaching/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://raphael-wagner.netlify.app/de/academia/teaching/</guid>
      <description>&lt;p&gt;Von meinem dritten Bachelorsemester an der Universität Ulm an habe ich Tutorien für andere Bachelorstudierende in Mathematik- und Informatikstudiengängen gehalten und Übungsblätter korrigiert. Die zugehörigen Vorlesungen beschäftigten sich meistens mit Grundlagen der Mathematik wie Analysis und Lineare Algebra.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/teaching/LA_Analysis_huaf2fc3b8a14810c73045d8c12286e060_3185594_531335d2baa3a56b0d6f15fd923ff7b5.webp 400w,
               /de/academia/teaching/LA_Analysis_huaf2fc3b8a14810c73045d8c12286e060_3185594_17f35835c1105d1791f471f171bf1d87.webp 760w,
               /de/academia/teaching/LA_Analysis_huaf2fc3b8a14810c73045d8c12286e060_3185594_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/teaching/LA_Analysis_huaf2fc3b8a14810c73045d8c12286e060_3185594_531335d2baa3a56b0d6f15fd923ff7b5.webp&#34;
               width=&#34;760&#34;
               height=&#34;379&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Meine beiden Semester an der Syracuse University wurden teilweise durch eine Teaching Assistantship finanziert, in deren Rahmen ich drei bis vier Tutorien pro Woche über Calculus II, III gehalten und beim Calculus Help Desk mitgeholfen habe.&lt;/p&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-syracuse&#34; href=&#34;https://raphael-wagner.netlify.app/media/albums/syracuse/syracuse01.jpg&#34; &gt;
      &lt;img src=&#34;https://raphael-wagner.netlify.app/media/albums/syracuse/syracuse01_hu4b236bb8c932d5ca11d05ac2e8d759d5_366692_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;syracuse01.jpg&#34; width=&#34;563&#34; height=&#34;750&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-syracuse&#34; href=&#34;https://raphael-wagner.netlify.app/media/albums/syracuse/syracuse02.jpg&#34; &gt;
      &lt;img src=&#34;https://raphael-wagner.netlify.app/media/albums/syracuse/syracuse02_hu7cc9c0e071d596f68c86c305ffea5521_225370_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;syracuse02.jpg&#34; width=&#34;750&#34; height=&#34;563&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;In meiner Position als Doktorand und wissenschaftlicher Mitarbeiter an der Universität gehört zu meinen Aufgaben die Leitung des Übungsbetriebes von ein oder zwei Vorlesungen pro Semester in der Form von:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Erstellung wöchentlicher Aufgabenblätter und der Klausuren,&lt;/li&gt;
&lt;li&gt;Organisation von Tutorien und der Korrektur der Blätter,&lt;/li&gt;
&lt;li&gt;Vorstellen der Lösungen im Hörsaal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Meine Lehrtätigkeit in der Regel für Veranstaltungen des Grundstudiums Mathematik der ersten vier Semester, so habe ich doch auch ein paar fortgeschrittenere Veranstaltung mitbetreut:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uni-ulm.de/en/ws20-1/hyperbolic-conservation-laws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyperbolic conservation laws&lt;/a&gt; (gelesen von Prof. Dr. Emil Wiedemann)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uni-ulm.de/en/mawi/iaa/lehre/ss-23/elements-of-calculus-of-variations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elements of the calculus of variations&lt;/a&gt; (gelesen von Dr. Nicola Zamponi)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uni-ulm.de/en/mawi/iaa/lehre/ws-23-24/functional-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Functional analysis (for data science)&lt;/a&gt; (gelesen von Prof. Dr. Anna Dall’Acqua)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Für eine vollständige Übersicht der Veranstaltungen, die ich mitbetreut habe, verweise ich auf &lt;a href=&#34;https://www.uni-ulm.de/mawi/iaa/members/raphael-wagner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;meine Universitäts-Webseite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Während der COVID-19 Pandemie habe ich meine Übungen im Hörsaal aufgezeichnet. Für einen ersten Eindruck meiner Lehre habe ich einen kleinen Abschnitt einer Übung hier hochgeladen (Analysis 1, Wintersemester 2020/2021).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Problem: Zeigen Sie die folgende Aussagen mittels vollständiger Induktion.
Für jede natürliche Zahl $n\in\mathbb{N}$ ergibt die Summe der Quadrate der ersten $n$ natürlichen Zahlen $\frac{1}{6}n(n+1)(2n+1)$.&lt;/em&gt;&lt;/p&gt;
&lt;video src=&#34;exercise_excerpt.mp4&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Forschung</title>
      <link>https://raphael-wagner.netlify.app/de/academia/research/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://raphael-wagner.netlify.app/de/academia/research/</guid>
      <description>&lt;p&gt;In meiner Forschung studiere ich verschiedene Begriffe statistischer Lösungen für gewisse Gleichungen in der mathematischen Strömungsmechanik. Während die Entwicklung dieser Theorie tendenziell sehr abstrakt angelegt ist, stammt die Motivation dafür aus der Untersuchung von Experimenten turbulenter Strömungen. Es ist in der Tat seit langem ein Paradigma in der Turbulenztheorie, dass solche Strömungen besser beschrieben werden können durch probabilistische und statistische Modelle, anstelle von rein deterministischen Überlegungen. So ist gerade einer der fundamentalen Eigenschaften einer turbulenten Strömung die Schwierig- bzw. Unmöglichkeit einer präzisen deterministschen Vorhersage ihres Verhaltens.&lt;br&gt;
Daher haben in den angewandten Wissenschaften Modelle turbulenter Strömungen in der Regel stets eine stchastische Komponente und betrachten häufig Mittel von relevanten Größen (Geschwindigkeit, Vortizität, etc.) in deren Beschreibung.&lt;br&gt;
Nachfolgend sind zwei Signale dargestellt mit denselben Parametern, welche aus einer Matlab Implementierung von E. Cheynet&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; des &lt;a href=&#34;https://en.wikipedia.org/wiki/Von_K%C3%A1rm%C3%A1n_wind_turbulence_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;von Kármán Wind-Turbulenz Modells&lt;/a&gt; stammen. Dieses Modell basiert auf den Reynolds-averaged Navier-Stokes (RANS) Gleichungen und hat sich als recht verlässliche Basis für die Beschreibung von Wind-Turbulenz bewiesen.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34;
           src=&#34;https://raphael-wagner.netlify.app/de/academia/research/signal_combined_01.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34;
           src=&#34;https://raphael-wagner.netlify.app/de/academia/research/signal_combined_02.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Wenn wir nachfolgend die finalen Histogramme über alle Geschwindigkeiten, die in der Simulation aufgetreten sind betrachten, stellen wir fest, dass diese eine gewisse Ähnlichkeit haben. Natürlich ist unsere Stichprobe sehr klein und die Simulation wurde nur zweimal wiederholt. Noch wichtiger ist es, dass ich betone, dass dieses Modell und dessen Implementierung bereits eine stochastische Komponente beinhalten, sodass die oben stehenden Histogramme nicht allzu überaschend sind. Allerdings werden solche Signale sehr ähnlich in echten Messungen in Windtunneln aufgezeichnet, wie man etwa in U. Frisch&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; sehen kann.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/research/histograms_hud68d7bf67082ec4f60eb1797e7371f18_77746_7177292ce088e68849c5414062dd34ee.webp 400w,
               /de/academia/research/histograms_hud68d7bf67082ec4f60eb1797e7371f18_77746_e4e8e36b8b88fa6f9660978afcebac4c.webp 760w,
               /de/academia/research/histograms_hud68d7bf67082ec4f60eb1797e7371f18_77746_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/research/histograms_hud68d7bf67082ec4f60eb1797e7371f18_77746_7177292ce088e68849c5414062dd34ee.webp&#34;
               width=&#34;760&#34;
               height=&#34;329&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Alternative und probabilistische Ansätze für Modelle und Gleichungen aus der Strömungsmechanik haben sich aus Tatsache heraus entwickelt, dass die Existenz eindeutiger, physikalisch relevanter Lösungen für diese Gleichungen oftmals unbekannt ist. Sogar eines der 1 Mio. $ &lt;a href=&#34;https://www.claymath.org/millennium-problems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Milleniumsprobleme&lt;/a&gt; bezieht sich auf diese Problematik.&lt;/p&gt;
&lt;p&gt;Im Folgenden werde ich etwas tiefer mein Forschungsgebiet beschreiben und welche Probleme ich in meinen Fachartikeln und Vorveröffentlichungen betrachtet habe.&lt;/p&gt;
&lt;p&gt;In meiner Forschung studiere ich primär die inkompressiblen Euler Gleichungen&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\partial_t u + (u\cdot\nabla)u + \nabla p + \gamma u &amp;amp;=  f,\
\operatorname{div} u &amp;amp;= 0,
\end{split}
\end{equation}
$$
sowie die inkompresseiblen Navier-Stokes Gleichungen
$$
\begin{equation}
\begin{split}
\partial_t u + (u\cdot\nabla)u + \nabla p + \gamma u - \nu\Delta u &amp;amp;=  f,\
\operatorname{div} u &amp;amp;= 0,
\end{split}
\end{equation}
$$
für ein Geschwindigkeitsfeld $u \colon (0,T) \times \Omega \to \mathbb{R}^d$ mit Druck $p\colon (0,T) \times \Omega \to \mathbb{R}$, einer externen Kraft $f\colon (0,T) \times \Omega \to \mathbb{R}^d$ und kinematischer Viskosität $\nu &amp;gt; 0$ auf einem Gebiet $\Omega \subset \mathbb{R}^d$ bis zu einer Zeit $T &amp;gt; 0$.&lt;br&gt;
Beide Gleichungen sind fundamental in der Strömungsmechanik, doch zugleich sind mathematisch viele Standardfragen und Probleme in der Theorie partieller Differentialgleichungen wie etwa &lt;em&gt;Existenz&lt;/em&gt;, &lt;em&gt;Eindeutigkeit&lt;/em&gt;, und &lt;em&gt;Regularität&lt;/em&gt; noch immer ungelöst. Ich möchte daher zunächst kurz erklären, was diese Eigenschaften bedeuten und ein paar damit verbundene offene Probleme nennen.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Existenz: Für jeden Anfangszustand des Fluids und anderen fixen Parametern wie etwa Randdaten hätten wir gerne, dass mathematisch eine Lösung zu den relevanten Fluidgleichungen gibt. Schließlich beschreiben wir ja ein physikalisches System und würden dies entsprechend erwarten.&lt;br&gt;
Existenz sogenannten schwacher Lösungen der 3D Navier-Stokes Gleichungen ist lange bekannt seit der Arbeit von Leray in 1934. Für die 2D Euler Gleichungen konnten viele (schwache) Existenzresultate in den letzten 15 Jahren mit der Methode &lt;a href=&#34;https://annals.math.princeton.edu/2009/170-3/p09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;konvexer Integration&lt;/a&gt; von De Lellis and Székelyhidi Jr. ausgestellt werden.&lt;br&gt;
Existenz physikalisch relevanter Lösungen innerhalb eines beliebigen Zeitintervalls ist allerdings nach wie vor im Allgemeinen unbekannt. Ich gehe darauf gleich noch im Punkt &lt;em&gt;Regularität&lt;/em&gt; etwas ein.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Eindeutigkeit: Idealerweise sollte gefunde Lösungen eindeutig sein. Schließlich würde man erwarten, dass wenn alle Parameter sowie Anfangs-und Randdaten festgelegt sind, das Verhalten des Fluids eindeutig bestimmbar sein sollte.&lt;br&gt;
Allerdings wurde gerade erst im vergangenen jahr von &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-mathematics/volume-196/issue-1/Non-uniqueness-of-Leray-solutions-of-the-forced-Navier-Stokes/10.4007/annals.2022.196.1.3.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Albritton, Brué and Colombo&lt;/a&gt; gezeigt, dass wür eine gewisse externe Kraft $f$, die Leray-Lösungen in der Tat uneindeutig sind. Es also zwei unterschiedliche Lösungen zu denselben Paramatern und Daten gibt. Darüber Hinaus produziert die zuvor erwähnte Methode konvexer Integration zu festen Daten der 2D Euler Gleichungen geradezu unendlich viele Lösungen. Zur Lösung dieses Problems ist es daher ein aktuelles Forschungsproblem weitere Bedingungen zu finden, mittels welcher man aus den vielen Lösungen eine (oder evtl. mehrere ähnliche) Lösungen herausfiltern kann, welche physikalisch am relevantesten erscheint.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regularität bedeutet, dass wenn die Daten und Parameter gute Eigenschaften haben wie man sie von echten Fluiden in Experimenten und Anwendungen erwarten würde, die mathematische Lösung ebenfalls diese Eigenschaften beibehält.&lt;br&gt;
In Kontrast zu den zuvor genannten schwachen Lösungen sind solche regulären Lösungen im allgemeinen stets eindeutig. Allerdings ist für diese oft die Existenzfrage ungeklärt bzw. ein offenes mathematisches Problem. Falls für die 3D Navier-Stokes Gleichungen dies falsch wäre, würde dies bedeuten, dass ein Fluid mit guten, physikalisch relevanten Parametern und Anfangs- und Randdaten in endlicher Zeit unendliche Geschwindigkeit entwickeln kann. Während dies physikalisch natürlich unmöglich ist, konnte dies mathematisch noch immer nicht ausgeschlossen werden und ist in eines der &lt;a href=&#34;https://www.claymath.org/millennium-problems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Milleniumsprobleme&lt;/a&gt; des Clay Mathematics Institute, welches als eines der bedeutendsten offenen mathematischen Probleme aufgefasst wird mit einem Preis dotiert auf eine Million USD.&lt;br&gt;
Dieses Problem besteht gleichermaßen für die 3D Euler Gleichungen.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Die Euler Gleichungen oder Navier-Stokes Gleichungen mit sehr kleiner Viskosität $\nu &amp;gt; 0$ (bzw. großer Reynolds-Zahl) werde generell assoziiert mit turbulenten Strömungen, da ein solcher Parameter bedeuter, dass die Teilchen bzw. Moleküle des Fluids sich unabhängiger voneiner bewegen können.&lt;br&gt;
Im Gegensatz zu der obigen Beschreibung der Eindeutigkeits-Eigenschaft ist ein fundamentaler Aspekt turbulenter Strömungen, dass diese eben nicht eindeutig bestimmtes bzw. bestimmbares Verhalten haben, weswegen es hier im Gegenzug zu vielen anderen Problemen der Physik tatsächlich nicht zwingend sinnvoll erscheint, Eindeutigkeit von Lösungen zu erwarten. Entsprechend sollte ein mathematisches Modell hier bewusst die Möglichkeit erlauben, dass sich das System auf mehrere Weisen in der Zeit entwickelt.&lt;br&gt;
Immerhin muss dies nicht zwingend in totalem Chaos enden, da zumindest experimentell anerkannt ist, dass gewisse statistische Größen turbulenter Strömungen in der Tat reproduzierbar sind.&lt;br&gt;
Ein Ansatz besteht nun darin, das System als grundsätzlich deterministisch aufzufassen und Abweichungen davon in der Form von zufälligem Rauschen zu beschreiben. Dies führt auf ein Modell basierend auf stochastischen Differentialgleichungen.&lt;br&gt;
Davon abweichend besteht ein anderer Ansatz darin, anstelle der Beschreibung einer einzelnen Lösung ein ganzes Ensemble von möglichen Lösungen oder möglichen Zuständen auf einem geeigneten Phasenraum über mögliche Wahrscheinlichkeitsverteilungen zu beschreiben. Anschließend studiert man die Evolution dieser Verteilungen in der Zeit.&lt;br&gt;
Ein etwas loser Ansatz dieser Art kann bereits in einer Arbeit von 1950 von Hopf gefunden werden und in einem präzisen Framework&lt;br&gt;
&lt;a href=&#34;http://www.numdam.org/item/RSMUP_1972__48__219_0.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1972&lt;/a&gt; und &lt;a href=&#34;http://www.numdam.org/item/RSMUP_1973__49__9_0.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1973&lt;/a&gt; von Foias und &lt;a href=&#34;https://link.springer.com/article/10.1007/BF00973601&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1978&lt;/a&gt; von Vishik und Fursikov für die 3D Navier-Stokes Gleichungen.&lt;br&gt;
Basierend auf fortführendem Studium dieser statistischen Lösungen für die Navier-Stokes Gleichungen war ein Hauptbestandteil meiner Promotion die Entwicklung analoger Lösungskonzepte für die 2D Euler Gleichungen.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0022123622003974?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wagner, R.,Wiedemann, E.: Statistical solutions of the two-dimensional incompressible
Euler equations in spaces of unbounded vorticity. J. Funct. Anal. 284(4), 109777 (2023)&lt;/a&gt;&lt;br&gt;
Mein erster publizierter Fachartikel, in Zusammenarbeit mit meinem Betreuer Prof. Dr. Emil Wiedemann diskutiert und zeigt die Existenz statistischer Lösungen der 2D Euler Gleichungen unter gewissen Annahmen an die Vortizität für mehrere Begriffe statistischer Lösungen der 2D Euler Gleichungen.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s00021-023-00800-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gallenmüller, D., Wagner, R. &amp;amp; Wiedemann, E.: Probabilistic Descriptions of Fluid
Flow: A Survey. J. Math. Fluid Mech. 25, 52 (2023)&lt;/a&gt;&lt;br&gt;
Mein zweiter publizierter Artikel, in Zusammenarbeit mit meinem Betreuer Prof. Dr. Emil Wiedemann und ehemaligem Postdoktorand Dr. Dennis Gallenmüller am Institut für Angewandte Analysis, zeigt Verbindungen zwischen verschiedenen Begriffen statistischer Lösungen auf, sowie den Zusammenhang mit anderen Konzepten wie etwa maßwertigen Lösungen und präsentiert allgemeine Strategien um die Existenz von Lösungen zu zeigen. Zuletzt werden noch kurz offene Probleme diskutiert.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.05081&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R. Wagner, Vanishing of long time average p-enstrophy dissipation rate in the inviscid
limit of the 2D damped Navier-Stokes equations, arXiv preprint, 2023&lt;/a&gt;&lt;br&gt;
Mein jüngster Artikel, welcher aktuell in der Begutachtung ist, verallgemeinert und vereinfacht ein früheres Resultat von 2007 von Constantin und Ramos über das Verschwinden von Langzeitmitteln der Enstrophiedissipationsrate im Viskositätslimes, für positive Dämpfung $\gamma &amp;gt; 0$.&lt;br&gt;
Enstrophie ist das Integral über dem Raum des Quadrats der Vortizität und damit ein Größe zur Beschreibung wie rotationell das Fluid über den Raum verteilt ist.&lt;br&gt;
Die Navier-Stokes und Euler Gleichungen oben unterscheiden sich lediglich durch den Viskositätsterm $\nu\Delta u$. Viskosität führt generell zu Dissipation von Enstrophie über die Zeit hinweg. Was passiert nun, wenn wir immer kleiner werdende Viskosität betrachten $(\nu \to 0)$? Wird auch die Rate der Enstrophiedissipation durch viskose Effekte immer kleiner und nähert sich $0$ an? Dies ist eine wichtige Frage in der Batchelor-Kraichnan Theorie von 2D Turbulenz. Falls eine Art Dissipation im System bestehen bleibt, nennt man dies typischerweise anomale Enstrophie-Dissipation.&lt;br&gt;
Wie bei Constantin und Ramos betrach ich in meinem Artikel zunächst Langzeitmittel des Systems um eine Art stationären Zustand zu erreichen und betrachte im Anschluss den Viskositätslimes $(\nu \to 0)$. Im Artikel benutze  ich dann ein paar nette Ideen aus der Ergodentheorie und der Theorie dynamischer Systeme sowie ein paar jüngere Resultate über den Viskositätslimes.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Cheynet, E. Wind Field Simulation (the Fast Version). Zenodo, 2020, doi:10.5281/ZENODO.3774136&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Frisch, U. Turbulence: The Legacy of A. N. Kolmogorov. Cambridge: Cambridge University Press; 1995. doi:10.1017/CBO9781139170666&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Maschinelles Lernen</title>
      <link>https://raphael-wagner.netlify.app/de/academia/ml/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://raphael-wagner.netlify.app/de/academia/ml/</guid>
      <description>&lt;p&gt;Ich bin das erste mal mit der Theorie maschinellen Lernens in einer Mastervorlesung über Pattern Recognition im Rahmen meines Informatik Nebensfachs gekommen.&lt;br&gt;
Das Gebiet beschäftigt sich mit der Entwicklung von Prozessen und Algorithmen, welche Computer befähigen, Aufgaben durch Erfahrung zu erlernen, gesteuert durch ein Performance-Maß. Insbesondere ist die erlernte Durchführung der Aufgabe nicht hartkodiert. In der Tat is es für komplexe Modelle wie tiefe neuronale Netze schwierig, Gesetztmäßigkeiten nachzuvollziehen, nach welchen dieses handelt.&lt;/p&gt;
&lt;p&gt;Mittlerweile haben meiner Einschätzung nach die meisten Akademiker gewisse Berührpunkte mit maschinellen Lernen gemacht aufgrund des breiten Anwendungsspektrums maschineller Lernalgorithmen und deren große Erfolge, z.B. bei der Erkennung Kreditkartenbetrugs in Kontoauszügen, oder der Erkennung von Tumoren in Gehirnscans.&lt;br&gt;
Selbst in der numerischen Analysis partieller Differentialgleichungen etablieren sich allmählich Algorithmen maschinellen Lernens. Insbesondere in den Geowissenschaften, in welchen große Mengen an Daten aufgezeichnet werden, erfolgt die Simulation von Modellen nicht mehr nur auf Basis der numerischen Berechnung der involiverten Gleichungen der Physik, sondern wird in der Regel kombiniert mit datengetriebenen Ansätzen. Ich konnte darüber dankbarerweise zwei spannende Vorträge im Rahmen des Mathematischen Kolloquiums an der Universität Ulm von &lt;a href=&#34;https://www.uni-ulm.de/fileadmin/website_uni_ulm/mawi.inst.010/Abstract_Jakob_Runge.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Dr. Jakob Runge&lt;/a&gt; und &lt;a href=&#34;https://www.uni-ulm.de/fileadmin/website_uni_ulm/mawi.inst.010/Abstract_Kutyniok.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Dr. Gitta Kutyniok&lt;/a&gt; besuchen, welche Experten in diesem Gebiet sind.&lt;/p&gt;
&lt;p&gt;Mittlerweile stehen eine Vielzahl an Büchern und anderen Ressourcen zur Verfügung, mit deren Hilfe und einem grundlegenden Verständnis von Statistik und Programmierung, man viele Methoden maschinellen Lernens erlernen kann. In jüngerer Zeit habe ich mich hier insbesondere mit &lt;a href=&#34;https://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow&lt;/a&gt; von Aurélien Géron auseinandergesetzt, welches ich als sehr zugänglich empfinde.&lt;/p&gt;
&lt;p&gt;Legt man die Faszination der Anwendungen für einen Moment auf die Seite, bleibt nach wie vor eine recht tiefe und interessante Theorie, welche vielen Methoden zugrundeliegt. Ein Beispiel, welchem ich kürzlich begegnet bin, ist das der Reproducing Kernel Hilbert Spaces (RKHS), welches ich gerne als Beispiel hier etwas illustrieren möchte.&lt;br&gt;
Der nette Aspekt der Theorie ist, dass die Theorie der RKHS nicht nur erklärt, warum Methoden wie Ridge Regression funkionieren und sinnvoll sind, sie erklärt auch, warum Anwender die tiefe Theorie nicht einmal verstehen müssen, da sie rein im Hintergrund abläuft.&lt;/p&gt;
&lt;p&gt;RKHS spielen eine Rolle für gewisse Regressions- und Klassifizierungsprobleme. Nehmen wir an, wir haben Input-Daten die in einer Menge $X$ enthalten sind. Dabei kann $X$ eine Menge von Textdateien sein, numerische Werte enthalten, Bilder etc. oder Kombinationen von diesen. Bei Klassifizierungsproblemen nehmen wir an, dass jedes Element in $X$ eine &lt;em&gt;wahre&lt;/em&gt; Klasse hat, z.B. wenn $X$ eine Menge von Katzen- und Hundebildern ist und jedes Bild einer der beiden Klasse angehört: Katze oder Hund.&lt;br&gt;
Bei Regressionsproblemen wird angenommen, dass jedes Element in $X$ einen &lt;em&gt;wahren&lt;/em&gt; oder &lt;em&gt;akkuraten&lt;/em&gt; Werte hat, wie z.B. der Wert eines Hauses, wobei $X$ Daten von Häusern bzw. Immobilien in einer Region enhält.&lt;br&gt;
Diese Klassen oder wahren Werte nennt man in der Regel label.&lt;/p&gt;
&lt;p&gt;Angenommen wir haben Daten $x_1, &amp;hellip; , x_n \in X$ für $n \in \mathbb{N}$ deren labels $y_1, &amp;hellip; , y_n \in \mathbb{R}$ wir kennen. Dann nennen wir $(x_1,y_1),&amp;hellip;,(x_n,y_n)$ unsere Trainingsdaten.&lt;/p&gt;
&lt;p&gt;Wir möchten dann relativ präzise den Preis eines jeden Hauses mit Daten mit den Eigenschaften $x \in X$ bestimmen. Wir beschreiben dies über eine Funktion $f\colon X \to \mathbb{R}$ und hoffen, dass durch Wahl einer Funktion $f$, welche die Trainingsdaten gut approximiert, wir mittels $f$ auch relativ präzise die label neuer, nicht in den Trainingsdaten enthaltener Input-Daten bestimmen können (das Overfitting-Problem spreche ich in einem Moment an).&lt;br&gt;
Wir messen die Präzision in der Trainingsphase durch Wahl eines Performance-Maßes $V\colon X \times \mathbb{R} \to [0,\infty)$, wobei $V(f(x_i),y_i)$ klein sein sollte, wenn $f(x_i) \sim y_i$ und groß, wenn $f(x_i)$ und $y_i$ stark voneinander abweichen. Eine Standardbeispiel eines Performance-Maßes ist die quadratische Distanz
$$ V(f(x_i),y_i) = |f(x_i) - y_i|^2.$$
Betrachten wir das Mittel über den Trainingsdaten, bekommen erhalten wir eine sogenannte Verlustfunktion (engl. loss function) für unsere Regressionsfunktion $f\colon X \to \mathbb{R}$:
$$ L(f) = \frac{1}{n}\sum_{i=1}^n V(f(x_i),y_i). $$&lt;/p&gt;
&lt;p&gt;Es gibt viele Möglichkeiten in der präzisen Wahl eines Performance-Maßes und der Verlustfunktion. Eine wichtige Eigenschaft, welche diese üblicherweise gemeinsam haben, ist die der (strikten) Konvexität, welche unter weiteren Annahmen zumindest in der Theorie die mathematische Existenz eines eindeutigen Minimums und die Konvergenz von Approximationsverfahren gegen dieses Minimum garantiert.&lt;/p&gt;
&lt;p&gt;In der Regel, durch eine vorläufige Untersuchung der zur Verfügung stehenden Trainingsdaten, hat man bereits a priori eine Vorstellung davon, wie einfach oder komplex der Zusammenhang zwischen den Daten $x \in X$ und den zugehörigen label $y \in \mathbb{R}$ ist. Darauf basierend kann man eine Vorabauswahl eines oder mehrerer allgemeiner Modelle treffen, z.B. lineare oder polynomielle Modelle, neuronal Netze usw. Daher, anstelle alle Funktionen $f\colon X \to \mathbb{R}$ als mögliche Regressionsfunktionen zu betrachten, schränken wir uns auf eine gewisse Teilmenge von Funktion $H \subset \lbrace f\colon X \to \mathbb{R} \rbrace$ ein.
Das Regressionsproblem besteht dann darin,
$$ f^* := \underset{f \in H}{\operatorname{argmin}} L(f) $$
zu bestimmen.&lt;/p&gt;
&lt;p&gt;Minimierungsprobleme in Hilberträumen sind tendenziell sehr elegant lösbar. Warum? Hilberträume sind gewisse Vektorräume mit einem Skalarprodukt. Ein solches Skalarprodukt $\langle\cdot,\cdot\rangle_H$ gibt uns einen Begriff von Orthogonalität, welcher eng verbunden ist mit dem Problem zur Minimierung von Abständen.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-die-kürzeste-strecke-zwischen-x-und-dem-liniensegment-l-is-durch-das-geradenstück-gegeben-welches-durch-x-und-l-verläuft-und-l-orthogonal-schneidet&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_870f9016b05d392c3f1348a7cccb2e75.webp 400w,
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_dd1c8d2ca012c7e3e2ae0e0cd4b1a8aa.webp 760w,
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_870f9016b05d392c3f1348a7cccb2e75.webp&#34;
               width=&#34;487&#34;
               height=&#34;347&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Die kürzeste Strecke zwischen $x$ und dem Liniensegment $L$ is durch das Geradenstück gegeben, welches durch $x$ und $L$ verläuft und $L$ orthogonal schneidet.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Daher hätten wir gerne eine Hilbertraumstruktur für unsere Menge von Funktionen $H$. Diese Sichtweise ist relativ abstrakt, da wir Funktionen in $H$ ähnlich zu Punkten in der Ebene $\mathbb{R}^2$ auffassen, wo wir einen &lt;em&gt;natürlichen&lt;/em&gt; Begriff von Orthogonalität haben. Dieser Ansatz ist aber fundamental in der &lt;a href=&#34;https://de.wikipedia.org/wiki/Funktionalanalysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Funktionalanalysis&lt;/a&gt;, welche im letzten Jahrhundert sich zu einer der mächtigsten Theorien in der mathematischen Analysis entwickelt hat.&lt;/p&gt;
&lt;p&gt;Allerdings entsteht nun die Frage, wie wir ein sinnvolles Konzept von Orthogonalität zwischen Funktionen in $H$ erhalten können. Dies ist in keiner Weise offensichtlich. Allerdings vereinfacht die Theorie der RKHS diese Frage signifikant in Form des &lt;a href=&#34;https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#Moore%E2%80%93Aronszajn_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moore-Aronszajn Theorems&lt;/a&gt;:
Angenommen wir haben einen Kern $K\colon X \times X \to \mathbb{R}$, mit den Eigenschaften:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Symmetrie: $K(z_1,z_2) = K(z_2,z_1)$ für alle $z_1,z_2 \in X$;&lt;/li&gt;
&lt;li&gt;Positive Semidefinitheit: Für alle $z_1,&amp;hellip;z_k \in X$, $\lambda_1,&amp;hellip;,\lambda_k \in \mathbb{R}$ gilt
$$ \sum_{i,j=1}^k \lambda_i\lambda_j K(z_i,z_j) \geq 0.$$
Dann gibt es einen eindeutigen Hilbertraum $H \subset \lbrace f \colon X \to \mathbb{R}\rbrace$, der &lt;em&gt;Reproducing Kernel Hilbert Space&lt;/em&gt;, dessen reproduzierender Kern $K$ ist, das bedeutet, dass für alle $f \in H$ und $x \in X$,
$$ f(x) = \langle K(x,\cdot), f \rangle_H.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Daher erhält man bei Wahl eines Kerns mit diesen Eigenschaften automatisch (quasi im Hintergrund) einen bzw. den zugehörigen RKHS. Allerdings sollte erwähnt werden, dass noch die Wahl eines geeigneten Kerns getroffen werden muss.
Im allgemeinen sind Symmetrie und positive (Semi-)Definitheit Eigenschaften, welche ein Kern mit denen eines Skalarprodukts $ \langle \cdot, \cdot \rangle$ gemeinsam. Für ein Skalarprodukt beschreibt $\langle z_1,z_2 \rangle$ gerade die Länge von $z_1$, projiziert auf die Gerade welche von $z_2$ aufgespannt wird (sofern $z_2$ Länge 1 hat). Je näher also $z_1$ und $z_2$ in dieselbe Richtung zeigen, desto größer wird der Wert des Skalarprodukt. Man stellt sich daher im Allgemeinen den Kern $K(z_1,z_2)$ als eine Art Ähnlichkeitsmaß zwischen zwei Werten $z_1,z_2 \in X$ vor.&lt;/p&gt;
&lt;p&gt;Typische Beispiele, falls $X = \mathbb{R}^d$, sind z.B.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;der Gaussche Kern $K(z_1,z_2) = \exp\left( \frac{|z_1-z_2|^2}{\sigma^2} \right)$ für ein $\sigma^2 &amp;gt; 0$,&lt;/li&gt;
&lt;li&gt;oder polynomielle Kerne $K(z_1,z_2) = (z_1\cdot z_2+ 1)^l$ für $l \in \mathbb{N}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Falls $X$ nicht aus numerischen Daten in $\mathbb{R}^n$ (oder eines anderen Hilbertraumes besteht), kann man zunächst $X$ durch eine &lt;em&gt;feature map&lt;/em&gt;  $\varphi\colon X \to Y$ transformieren. Ist $Y$ ein Hilbertraum mit Skalarprodukt $\langle \cdot, \cdot \rangle_Y$, dann ist ein Kern durch
$$ K(x,y) = \langle \varphi(x),\varphi(y) \rangle_Y$$
gegeben. Diese Methode zur Konstruktion von Kernen über feature maps wird häufig benutzt, um nicht-lineare Regression in einem Raum niedriger Dimension in ein lineares Regressionsproblem in einem höherdimensionalen umzuformulieren.&lt;/p&gt;
&lt;p&gt;Wir diskutieren hier kurz diese Methode für Standardbeispiele quadratischer Regression. Angenommen unsere Daten $X \subset \mathbb{R}$ sind 1-dimensional und wir suchen nach einer quadratischen Regressionsfunktion
$$f \colon X \to \mathbb{R}, x \mapsto a x^2 + bx. $$
Mit Hilfe der der feature map $\varphi \colon \mathbb{R} \to \mathbb{R}^2, x \mapsto (x,x^2)$, verstecken wir das nicht-lineare Problem im Kern $K$, gegeben durch das Skalarprodukt auf $\mathbb{R}^2$ und der feature map $\varphi$, d.h.
$$K(z_1,z_2) = \langle \varphi(z_1),\varphi(z_2)\rangle_{\mathbb{R}^2} = \langle (z_1,z_1^2),(z_2,z_2^2)\rangle_{\mathbb{R}^2} = z_1z_2 + z_1^2z_2^2.$$
Der Ansatz über die feature map ist in sofern sinnvoll, da man sich überlegen kann, dass die quadratischen Funktionen auf $\mathbb{R}$ wie oben zu linearen Funktionen in $\mathbb{R}^2$ korrespondieren.&lt;br&gt;
Solche Beispiele fallen in die Kategorie von Methoden, welche typischerweise als &lt;em&gt;kernel trick&lt;/em&gt; bezeichnet werden.&lt;br&gt;
Nachfolgend findet sich ein Beispiel eine 1-dimensionalen Menge $X = G \cup B$ aufgeteilt in die beiden Klassen
$$G = \lbrace -1, -2 \rbrace \text{ und } B = \lbrace -3, 1, 2\rbrace,$$
welche nicht linear, aber quadratischen separierbar sind in dem Sinne, dass für
$y = f(x) = x^2+2.5x$, zum einen $G = \lbrace x \in X : f(x) &amp;lt; 0\rbrace$ und zum anderen $B = \lbrace x \in X : f(x) \geq 0\rbrace$ gilt.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-klassifizierung-von-1-dimensionalen-werten-in-die-beiden-klassen-grün-and-blau&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_67c685a391a32e6bacfe73f23cfe9617.webp 400w,
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_bdf7edae1f7a55bc03b0a131a225f0be.webp 760w,
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_67c685a391a32e6bacfe73f23cfe9617.webp&#34;
               width=&#34;545&#34;
               height=&#34;201&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Klassifizierung von 1-dimensionalen Werten in die beiden Klassen &lt;em&gt;Grün&lt;/em&gt; and &lt;em&gt;Blau&lt;/em&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Die feature map transformiert die Daten in $X$ in Punkte in der Ebene.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-transformation-über-die-feature-map-varphi&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_a78f25e2dee441fbce4cfa5ea1449262.webp 400w,
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_78e6e402960b24510fa30fcce68c584a.webp 760w,
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_a78f25e2dee441fbce4cfa5ea1449262.webp&#34;
               width=&#34;565&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Transformation über die feature map $\varphi$.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dort sind die Daten linear separierbar, falls z.B.
$$\tilde{G} = \lbrace (-1,1), (-2,4) \rbrace \text{ and } \tilde{B} = \lbrace (-3,9), (1,1), (2,4)\rbrace,$$
und $\tilde{f}(x,y) = y+2.5x$, dann gilt $\tilde{G} = \lbrace (x,y) = \varphi(x) : x \in X \text{ and } \tilde{f}(x,y) &amp;lt; 0\rbrace$, while $\tilde{B} = \lbrace (x,y) = \varphi(x) : x \in X \text{ and } \tilde{f}(x,y) \geq 0\rbrace$.&lt;/p&gt;
&lt;p&gt;Wir bemerken hier noch, dass wir $f$ sich auch als Linearkombination der Kernfunktionen $K(-3,\cdot), K(-2,\cdot), K(-1,\cdot), K(1,\cdot), K(2,\cdot)$, ausgewertet an den $x$-Werten der Trainingsdaten darstellen können:
$$f(x) = \frac{7}{4} K(1,x) - \frac{3}{4}K(-1,x).$$
Das ist kein bloßer Zufall und wird gleich etwas algemeiner beschrieben.&lt;/p&gt;
&lt;p&gt;An dieser Stelle möchte ich auch noch einmal betonen, dass der Satz von Moore-Aronszajn auf keinen Fall trivial ist und ein Verständnis seines Beweises eine solide Basis in Funkionalanalysis erfordert.&lt;/p&gt;
&lt;p&gt;Zurück zu unserem Regressionsproblem und dem RKHS $H$. Ein allgemein großes Problem besteht im Overfitting der Trainingsdaten, was passieren kann, wenn wir ein Modell aufstellen wollen, das sehr komplex ist relativ zu den eigentlichen Daten, welches dann zu sensitiv auf Rauschen reagiert.&lt;br&gt;
In unserer Situation eines RKHS kann dies durch &lt;em&gt;Tikhonov Regularisierung&lt;/em&gt; unterdrückt werden. Das bedeutet, wir fügen unserer Verlustfunktionen einen Regularisierungsterm hinzu, sodass
$$ L(f) = \frac{1}{n}\sum_{i=1}^n V(f(x_i),y_i) + c |f|_H^2,$$
wobei $c &amp;gt; 0$ ein Regularisierungsparameter und $|f|_H = \sqrt{\langle f,f\rangle_H}$ die Norm von $f$ ist, welche durch das Skalarprodukt auf $H$ gegeben ist. Das ist in der Tat eine Regularisierung in dem Sinne, dass jedes $f \in H$ Lipschitz-stetig mit Lipschitz-Konstanten $|f|_H$ ist:
\begin{align*}
|f(z_1) - f(z_2)| = |\langle f,K(z_1,z_2)-K(z_1,z_2)\rangle| \leq  |f|_H d(z_1,z_2),
\end{align*}
wobei $d(z_1,z_2) := |K(z_1,\cdot)-K(z_2,\cdot)|_H$ als Distanz zwischen $z_1$ und $z_2$ aufgefasst werden kann. Diese Lipschitzstetigkeit bedeutet, dass die Steigung von $f$ beschränkt ist durch $|f|_H$. Daher, durch Hinzufügen dieses Terms zu unserer Verlustfunktion, welche wir minimieren wollen, wir der Overfitting-Problem entgegen wirken. Die folgende Skizze illustriert dies. Die Beziehung zwischen einem Input $x$ und dem label $y$ wirkt linear (evtl. mit leichter Krümmung). Allerdings trifft die orangene Kurve die Datenpunkte perfekt, während die Verlustfunktion für die lineare Kurve in grün positiv wäre.&lt;br&gt;
Dabei ist allerdings zu beachten, dass die orangene Kurve eine recht große Steigung an gewissen Stellen aufweist, um alle Punkte perfekt zu treffen. Durch Hinzufügen des Regularisierungsterms zur Verlustfunktion, wird diese aber auch wie gewünscht groß für die orangene Kurve, wodurch das lineare Modell eventuell mehr evorzugt wird.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-overfitting-der-trainingsdaten&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_8af8154c84a1d4eab27cf9382d93654e.webp 400w,
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_c0820a743c6aad0e69a7e504e80b2ae5.webp 760w,
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_8af8154c84a1d4eab27cf9382d93654e.webp&#34;
               width=&#34;760&#34;
               height=&#34;394&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overfitting der Trainingsdaten
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Im allgemeinen kann es sehr schwierig bis hin zu unmöglich sein, den RKHS zu einem Kern $K$ zu bestimmen. Daher, selbst wenn sich das Minimierungsproblem in dem RKHS $H$ elegant lösen lässt , die Frage berechtigt, was der praktische Nutzen davon ist, wenn $H$ nicht explizit bekannt ist. Hier hilft erneu die abstrakte Theorie der RKHS, welche mittels des &lt;a href=&#34;https://en.wikipedia.org/wiki/Representer_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representer Theorems&lt;/a&gt; folgendes aussagt:&lt;/p&gt;
&lt;p&gt;In der gegebenen Situation mit Trainingsdaten $(x_i,y_i), i = 1,&amp;hellip;,n$ ist die optimale Lösung (sofern sie existiert), gegeben durch
$$ f^*(x) = \sum_{i=1}^n \lambda_i K(x_i,x) $$
für gewisse Koeffizienten $\lambda_1,&amp;hellip;,\lambda_n \in \mathbb{R}^n$.&lt;/p&gt;
&lt;p&gt;Dadurch müssen wir nach dem Minimierer also nur in der Menge der Linearkombination der Funktionen $x \mapsto K(x_1,x),&amp;hellip;,K(x_n,x)$ suchen, was essentiell lediglich lineare Regression in $\mathbb{R}^n$ ist.&lt;/p&gt;
&lt;p&gt;Um dies Zusammenzufassen werfen wir einen Blick auf die nachfolgende Abbildung. Das untenstehende Diagramm kommutiert, das bedeutet, dass wir um einen Minimierer des Regressionsproblems zu finden, anstelle des Weges über die abstrakte Theorie direkt lineare Regression in  $\mathbb{R}^n$ durchführen können um optimale Koeffizienten $\lambda_1, &amp;hellip; ,\lambda_n \in \mathbb{R}$ bezüglich der Verlustfunktion für die Funktion $f^*(x) = \sum_{i=1}^n \lambda_i K(x_i,x)$ zu bestimmen. Man nennt dieses Verfahren oft Ridge Regression.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-suche-nach-optimalen-regressionsfunktionen-mittels-rkhs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;image&#34; srcset=&#34;
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_838e7e14f963fbe49e6f88532f02678d.webp 400w,
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_3da1440d2f29948fd8eafcaf148f1989.webp 760w,
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_838e7e14f963fbe49e6f88532f02678d.webp&#34;
               width=&#34;760&#34;
               height=&#34;287&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Suche nach optimalen Regressionsfunktionen mittels RKHS
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Im folgenden habe ich ein kleines Beispiel für Ridge Regression angefügt in der Form eines Jupyter Notebook, welches illustriert, dass sich die Methode eignen kann, um nicht-lineare Zusammenhänge zu erkennen ohne zu sensitiv auf Rauschen zu reagieren. Dafür habe ich die bereits implementierten Methoden in scikit-learn verwendet.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.kernel_ridge&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KernelRidge&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GridSearchCV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;num_samples&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# generate the independent variable (x) as a random sample from a uniform distribution&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;low&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;high&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_samples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# generate the dependent variable (y) as sin(x) with some gaussian noise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scale&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_samples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ravel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# plot sample data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;X&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Nonlinear sample data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;   
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_35af086aceddd76e13bb89981be66b2f.webp 400w,
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_50a1299c6152702312cacbb8d9a93427.webp 760w,
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_35af086aceddd76e13bb89981be66b2f.webp&#34;
               width=&#34;579&#34;
               height=&#34;453&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Fit a ridge regression model with gaussian kernel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Use grid-search cross-validation to find good parameter combinations alpha (regularization) and gamma = 1/sigma&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kr_cv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;KernelRidge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kernel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;rbf&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;param_grid&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;alpha&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;gamma&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linspace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kr_cv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_plot&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linspace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[:,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;training data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kr_cv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;y = predicted_labels(x)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ravel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;y = sin(x) (&amp;#39;&amp;#39;true&amp;#39;&amp;#39; labels)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Kernel ridge regression&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;legend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend at 0x238f07d75e0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1344193a9fd31e6e1eab98c22c93a026.webp 400w,
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_4940ed2e2987ae24062590225aecb396.webp 760w,
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://raphael-wagner.netlify.app/de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1344193a9fd31e6e1eab98c22c93a026.webp&#34;
               width=&#34;579&#34;
               height=&#34;453&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Wie wir sehen, konnte unser Regressionsmodell in diesem Beispiel den nicht-linearen Zusammenhang zwischen $x$ und $y$ erkennen, ohne zu sehr durch die Störung bzw. das Rauschen beeinflusst zu werden.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
