<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: 16. Dezember 2023 --><html lang="de" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.9.3 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.bfcf0ec07d6a061d91610840efa1e36b.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  










  

<meta name="description" content="Ein kurzer Umriss gewisser Aspekte der Mathematik maschinellen Lernens, welche mich interessieren. Als Beispiel beschreibe ich ein paar Details zur Theorie der Reproducing Kernel Hilbert Spaces und der damit verbundenen Ridge-Regression." />



  <link rel="alternate" hreflang="en" href="https://raphael-wagner.netlify.app/academia/ml/" />

<link rel="alternate" hreflang="de" href="https://raphael-wagner.netlify.app/de/academia/ml/" />
<link rel="canonical" href="https://raphael-wagner.netlify.app/de/academia/ml/" />



  <link rel="manifest" href="/de/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu054d31ac38a623158e2d971b85a39826_2241_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu054d31ac38a623158e2d971b85a39826_2241_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://raphael-wagner.netlify.app/de/academia/ml/featured.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Raphael Wagner" />
<meta property="og:url" content="https://raphael-wagner.netlify.app/de/academia/ml/" />
<meta property="og:title" content="Maschinelles Lernen | Raphael Wagner" />
<meta property="og:description" content="Ein kurzer Umriss gewisser Aspekte der Mathematik maschinellen Lernens, welche mich interessieren. Als Beispiel beschreibe ich ein paar Details zur Theorie der Reproducing Kernel Hilbert Spaces und der damit verbundenen Ridge-Regression." /><meta property="og:image" content="https://raphael-wagner.netlify.app/de/academia/ml/featured.png" /><meta property="og:locale" content="de" />

  
    <meta
      property="article:published_time"
      content="2016-04-27T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2016-04-27T00:00:00&#43;00:00">
  







  




  
  
  

  
  

  


  
  <title>Maschinelles Lernen | Raphael Wagner</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="32c042111ded50ca96261c8cd438caec" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.9738f2447ad7945edbc62bca93fc890e.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Suche</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Suche..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Suche...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/de/">Raphael Wagner</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Navigation einblenden">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/de/">Raphael Wagner</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/de/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/de/#academia"><span>Akademische Interessen</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/de/#experience"><span>Berufserfahrung</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/de/#personal"><span>Freizeit</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/de/uploads/CV_Wagner_%28allgemein%29.pdf"><span>Lebenslauf</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Suche"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Einstellungen anzeigen">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Hell</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dunkel</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatisch</span>
            </a>
          </div>
        </li>
        

        
        
        <li class="nav-item dropdown i18n-dropdown">
          <a href="#" class="nav-link " data-toggle="dropdown"
             aria-haspopup="true" aria-label="语言">
            <i class="fas fa-globe mr-1" aria-hidden="true"></i></a>
          <div class="dropdown-menu">
            <div class="dropdown-item dropdown-item-active">
              <span>Deutsch</span>
            </div>
            
            <a class="dropdown-item" href="https://raphael-wagner.netlify.app/academia/ml/">
              <span>English</span>
            </a>
            
          </div>
        </li>
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  



<div class="article-container pt-3">
  <h1>Maschinelles Lernen</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    
  </span>
  

  

  

  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 579px; max-height: 453px;">
  <div style="position: relative">
    <img src="/de/academia/ml/featured_hub1a234681851d62ec836f4207e7cd568_39849_5c21021fda6af80ad11b78c0c959721c.webp" width="579" height="453" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Ich bin das erste mal mit der Theorie maschinellen Lernens in einer Mastervorlesung über Pattern Recognition im Rahmen meines Informatik Nebensfachs gekommen.<br>
Das Gebiet beschäftigt sich mit der Entwicklung von Prozessen und Algorithmen, welche Computer befähigen, Aufgaben durch Erfahrung zu erlernen, gesteuert durch ein Performance-Maß. Insbesondere ist die erlernte Durchführung der Aufgabe nicht hartkodiert. In der Tat is es für komplexe Modelle wie tiefe neuronale Netze schwierig, Gesetztmäßigkeiten nachzuvollziehen, nach welchen dieses handelt.</p>
<p>Mittlerweile haben meiner Einschätzung nach die meisten Akademiker gewisse Berührpunkte mit maschinellen Lernen gemacht aufgrund des breiten Anwendungsspektrums maschineller Lernalgorithmen und deren große Erfolge, z.B. bei der Erkennung Kreditkartenbetrugs in Kontoauszügen, oder der Erkennung von Tumoren in Gehirnscans.<br>
Selbst in der numerischen Analysis partieller Differentialgleichungen etablieren sich allmählich Algorithmen maschinellen Lernens. Insbesondere in den Geowissenschaften, in welchen große Mengen an Daten aufgezeichnet werden, erfolgt die Simulation von Modellen nicht mehr nur auf Basis der numerischen Berechnung der involiverten Gleichungen der Physik, sondern wird in der Regel kombiniert mit datengetriebenen Ansätzen. Ich konnte darüber dankbarerweise zwei spannende Vorträge im Rahmen des Mathematischen Kolloquiums an der Universität Ulm von <a href="https://www.uni-ulm.de/fileadmin/website_uni_ulm/mawi.inst.010/Abstract_Jakob_Runge.pdf" target="_blank" rel="noopener">Prof. Dr. Jakob Runge</a> und <a href="https://www.uni-ulm.de/fileadmin/website_uni_ulm/mawi.inst.010/Abstract_Kutyniok.pdf" target="_blank" rel="noopener">Prof. Dr. Gitta Kutyniok</a> besuchen, welche Experten in diesem Gebiet sind.</p>
<p>Mittlerweile stehen eine Vielzahl an Büchern und anderen Ressourcen zur Verfügung, mit deren Hilfe und einem grundlegenden Verständnis von Statistik und Programmierung, man viele Methoden maschinellen Lernens erlernen kann. In jüngerer Zeit habe ich mich hier insbesondere mit <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a> von Aurélien Géron auseinandergesetzt, welches ich als sehr zugänglich empfinde.</p>
<p>Legt man die Faszination der Anwendungen für einen Moment auf die Seite, bleibt nach wie vor eine recht tiefe und interessante Theorie, welche vielen Methoden zugrundeliegt. Ein Beispiel, welchem ich kürzlich begegnet bin, ist das der Reproducing Kernel Hilbert Spaces (RKHS), welches ich gerne als Beispiel hier etwas illustrieren möchte.<br>
Der nette Aspekt der Theorie ist, dass die Theorie der RKHS nicht nur erklärt, warum Methoden wie Ridge Regression funkionieren und sinnvoll sind, sie erklärt auch, warum Anwender die tiefe Theorie nicht einmal verstehen müssen, da sie rein im Hintergrund abläuft.</p>
<p>RKHS spielen eine Rolle für gewisse Regressions- und Klassifizierungsprobleme. Nehmen wir an, wir haben Input-Daten die in einer Menge $X$ enthalten sind. Dabei kann $X$ eine Menge von Textdateien sein, numerische Werte enthalten, Bilder etc. oder Kombinationen von diesen. Bei Klassifizierungsproblemen nehmen wir an, dass jedes Element in $X$ eine <em>wahre</em> Klasse hat, z.B. wenn $X$ eine Menge von Katzen- und Hundebildern ist und jedes Bild einer der beiden Klasse angehört: Katze oder Hund.<br>
Bei Regressionsproblemen wird angenommen, dass jedes Element in $X$ einen <em>wahren</em> oder <em>akkuraten</em> Werte hat, wie z.B. der Wert eines Hauses, wobei $X$ Daten von Häusern bzw. Immobilien in einer Region enhält.<br>
Diese Klassen oder wahren Werte nennt man in der Regel label.</p>
<p>Angenommen wir haben Daten $x_1, &hellip; , x_n \in X$ für $n \in \mathbb{N}$ deren labels $y_1, &hellip; , y_n \in \mathbb{R}$ wir kennen. Dann nennen wir $(x_1,y_1),&hellip;,(x_n,y_n)$ unsere Trainingsdaten.</p>
<p>Wir möchten dann relativ präzise den Preis eines jeden Hauses mit Daten mit den Eigenschaften $x \in X$ bestimmen. Wir beschreiben dies über eine Funktion $f\colon X \to \mathbb{R}$ und hoffen, dass durch Wahl einer Funktion $f$, welche die Trainingsdaten gut approximiert, wir mittels $f$ auch relativ präzise die label neuer, nicht in den Trainingsdaten enthaltener Input-Daten bestimmen können (das Overfitting-Problem spreche ich in einem Moment an).<br>
Wir messen die Präzision in der Trainingsphase durch Wahl eines Performance-Maßes $V\colon X \times \mathbb{R} \to [0,\infty)$, wobei $V(f(x_i),y_i)$ klein sein sollte, wenn $f(x_i) \sim y_i$ und groß, wenn $f(x_i)$ und $y_i$ stark voneinander abweichen. Eine Standardbeispiel eines Performance-Maßes ist die quadratische Distanz
$$ V(f(x_i),y_i) = |f(x_i) - y_i|^2.$$
Betrachten wir das Mittel über den Trainingsdaten, bekommen erhalten wir eine sogenannte Verlustfunktion (engl. loss function) für unsere Regressionsfunktion $f\colon X \to \mathbb{R}$:
$$ L(f) = \frac{1}{n}\sum_{i=1}^n V(f(x_i),y_i). $$</p>
<p>Es gibt viele Möglichkeiten in der präzisen Wahl eines Performance-Maßes und der Verlustfunktion. Eine wichtige Eigenschaft, welche diese üblicherweise gemeinsam haben, ist die der (strikten) Konvexität, welche unter weiteren Annahmen zumindest in der Theorie die mathematische Existenz eines eindeutigen Minimums und die Konvergenz von Approximationsverfahren gegen dieses Minimum garantiert.</p>
<p>In der Regel, durch eine vorläufige Untersuchung der zur Verfügung stehenden Trainingsdaten, hat man bereits a priori eine Vorstellung davon, wie einfach oder komplex der Zusammenhang zwischen den Daten $x \in X$ und den zugehörigen label $y \in \mathbb{R}$ ist. Darauf basierend kann man eine Vorabauswahl eines oder mehrerer allgemeiner Modelle treffen, z.B. lineare oder polynomielle Modelle, neuronal Netze usw. Daher, anstelle alle Funktionen $f\colon X \to \mathbb{R}$ als mögliche Regressionsfunktionen zu betrachten, schränken wir uns auf eine gewisse Teilmenge von Funktion $H \subset \lbrace f\colon X \to \mathbb{R} \rbrace$ ein.
Das Regressionsproblem besteht dann darin,
$$ f^* := \underset{f \in H}{\operatorname{argmin}} L(f) $$
zu bestimmen.</p>
<p>Minimierungsprobleme in Hilberträumen sind tendenziell sehr elegant lösbar. Warum? Hilberträume sind gewisse Vektorräume mit einem Skalarprodukt. Ein solches Skalarprodukt $\langle\cdot,\cdot\rangle_H$ gibt uns einen Begriff von Orthogonalität, welcher eng verbunden ist mit dem Problem zur Minimierung von Abständen.</p>
<p>















<figure  id="figure-die-kürzeste-strecke-zwischen-x-und-dem-liniensegment-l-is-durch-das-geradenstück-gegeben-welches-durch-x-und-l-verläuft-und-l-orthogonal-schneidet">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="image" srcset="
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_870f9016b05d392c3f1348a7cccb2e75.webp 400w,
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_dd1c8d2ca012c7e3e2ae0e0cd4b1a8aa.webp 760w,
               /de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_1200x1200_fit_q75_h2_lanczos.webp 1200w"
               src="/de/academia/ml/minimize_hu0f72dd4434aa2b2ee20cf4f9843ca179_11416_870f9016b05d392c3f1348a7cccb2e75.webp"
               width="487"
               height="347"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Die kürzeste Strecke zwischen $x$ und dem Liniensegment $L$ is durch das Geradenstück gegeben, welches durch $x$ und $L$ verläuft und $L$ orthogonal schneidet.
    </figcaption></figure>
</p>
<p>Daher hätten wir gerne eine Hilbertraumstruktur für unsere Menge von Funktionen $H$. Diese Sichtweise ist relativ abstrakt, da wir Funktionen in $H$ ähnlich zu Punkten in der Ebene $\mathbb{R}^2$ auffassen, wo wir einen <em>natürlichen</em> Begriff von Orthogonalität haben. Dieser Ansatz ist aber fundamental in der <a href="https://de.wikipedia.org/wiki/Funktionalanalysis" target="_blank" rel="noopener">Funktionalanalysis</a>, welche im letzten Jahrhundert sich zu einer der mächtigsten Theorien in der mathematischen Analysis entwickelt hat.</p>
<p>Allerdings entsteht nun die Frage, wie wir ein sinnvolles Konzept von Orthogonalität zwischen Funktionen in $H$ erhalten können. Dies ist in keiner Weise offensichtlich. Allerdings vereinfacht die Theorie der RKHS diese Frage signifikant in Form des <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#Moore%E2%80%93Aronszajn_theorem" target="_blank" rel="noopener">Moore-Aronszajn Theorems</a>:
Angenommen wir haben einen Kern $K\colon X \times X \to \mathbb{R}$, mit den Eigenschaften:</p>
<ul>
<li>Symmetrie: $K(z_1,z_2) = K(z_2,z_1)$ für alle $z_1,z_2 \in X$;</li>
<li>Positive Semidefinitheit: Für alle $z_1,&hellip;z_k \in X$, $\lambda_1,&hellip;,\lambda_k \in \mathbb{R}$ gilt
$$ \sum_{i,j=1}^k \lambda_i\lambda_j K(z_i,z_j) \geq 0.$$
Dann gibt es einen eindeutigen Hilbertraum $H \subset \lbrace f \colon X \to \mathbb{R}\rbrace$, der <em>Reproducing Kernel Hilbert Space</em>, dessen reproduzierender Kern $K$ ist, das bedeutet, dass für alle $f \in H$ und $x \in X$,
$$ f(x) = \langle K(x,\cdot), f \rangle_H.$$</li>
</ul>
<p>Daher erhält man bei Wahl eines Kerns mit diesen Eigenschaften automatisch (quasi im Hintergrund) einen bzw. den zugehörigen RKHS. Allerdings sollte erwähnt werden, dass noch die Wahl eines geeigneten Kerns getroffen werden muss.
Im allgemeinen sind Symmetrie und positive (Semi-)Definitheit Eigenschaften, welche ein Kern mit denen eines Skalarprodukts $ \langle \cdot, \cdot \rangle$ gemeinsam. Für ein Skalarprodukt beschreibt $\langle z_1,z_2 \rangle$ gerade die Länge von $z_1$, projiziert auf die Gerade welche von $z_2$ aufgespannt wird (sofern $z_2$ Länge 1 hat). Je näher also $z_1$ und $z_2$ in dieselbe Richtung zeigen, desto größer wird der Wert des Skalarprodukt. Man stellt sich daher im Allgemeinen den Kern $K(z_1,z_2)$ als eine Art Ähnlichkeitsmaß zwischen zwei Werten $z_1,z_2 \in X$ vor.</p>
<p>Typische Beispiele, falls $X = \mathbb{R}^d$, sind z.B.</p>
<ul>
<li>der Gaussche Kern $K(z_1,z_2) = \exp\left( \frac{|z_1-z_2|^2}{\sigma^2} \right)$ für ein $\sigma^2 &gt; 0$,</li>
<li>oder polynomielle Kerne $K(z_1,z_2) = (z_1\cdot z_2+ 1)^l$ für $l \in \mathbb{N}$.</li>
</ul>
<p>Falls $X$ nicht aus numerischen Daten in $\mathbb{R}^n$ (oder eines anderen Hilbertraumes besteht), kann man zunächst $X$ durch eine <em>feature map</em>  $\varphi\colon X \to Y$ transformieren. Ist $Y$ ein Hilbertraum mit Skalarprodukt $\langle \cdot, \cdot \rangle_Y$, dann ist ein Kern durch
$$ K(x,y) = \langle \varphi(x),\varphi(y) \rangle_Y$$
gegeben. Diese Methode zur Konstruktion von Kernen über feature maps wird häufig benutzt, um nicht-lineare Regression in einem Raum niedriger Dimension in ein lineares Regressionsproblem in einem höherdimensionalen umzuformulieren.</p>
<p>Wir diskutieren hier kurz diese Methode für Standardbeispiele quadratischer Regression. Angenommen unsere Daten $X \subset \mathbb{R}$ sind 1-dimensional und wir suchen nach einer quadratischen Regressionsfunktion
$$f \colon X \to \mathbb{R}, x \mapsto a x^2 + bx. $$
Mit Hilfe der der feature map $\varphi \colon \mathbb{R} \to \mathbb{R}^2, x \mapsto (x,x^2)$, verstecken wir das nicht-lineare Problem im Kern $K$, gegeben durch das Skalarprodukt auf $\mathbb{R}^2$ und der feature map $\varphi$, d.h.
$$K(z_1,z_2) = \langle \varphi(z_1),\varphi(z_2)\rangle_{\mathbb{R}^2} = \langle (z_1,z_1^2),(z_2,z_2^2)\rangle_{\mathbb{R}^2} = z_1z_2 + z_1^2z_2^2.$$
Der Ansatz über die feature map ist in sofern sinnvoll, da man sich überlegen kann, dass die quadratischen Funktionen auf $\mathbb{R}$ wie oben zu linearen Funktionen in $\mathbb{R}^2$ korrespondieren.<br>
Solche Beispiele fallen in die Kategorie von Methoden, welche typischerweise als <em>kernel trick</em> bezeichnet werden.<br>
Nachfolgend findet sich ein Beispiel eine 1-dimensionalen Menge $X = G \cup B$ aufgeteilt in die beiden Klassen
$$G = \lbrace -1, -2 \rbrace \text{ und } B = \lbrace -3, 1, 2\rbrace,$$
welche nicht linear, aber quadratischen separierbar sind in dem Sinne, dass für
$y = f(x) = x^2+2.5x$, zum einen $G = \lbrace x \in X : f(x) &lt; 0\rbrace$ und zum anderen $B = \lbrace x \in X : f(x) \geq 0\rbrace$ gilt.</p>
<p>















<figure  id="figure-klassifizierung-von-1-dimensionalen-werten-in-die-beiden-klassen-grün-and-blau">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="image" srcset="
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_67c685a391a32e6bacfe73f23cfe9617.webp 400w,
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_bdf7edae1f7a55bc03b0a131a225f0be.webp 760w,
               /de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_1200x1200_fit_q75_h2_lanczos.webp 1200w"
               src="/de/academia/ml/1D_nonseparable_hu91562f7b31c0850cf1ff273429f56a24_11714_67c685a391a32e6bacfe73f23cfe9617.webp"
               width="545"
               height="201"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Klassifizierung von 1-dimensionalen Werten in die beiden Klassen <em>Grün</em> and <em>Blau</em>.
    </figcaption></figure>
</p>
<p>Die feature map transformiert die Daten in $X$ in Punkte in der Ebene.</p>
<p>















<figure  id="figure-transformation-über-die-feature-map-varphi">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="image" srcset="
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_a78f25e2dee441fbce4cfa5ea1449262.webp 400w,
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_78e6e402960b24510fa30fcce68c584a.webp 760w,
               /de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_1200x1200_fit_q75_h2_lanczos.webp 1200w"
               src="/de/academia/ml/1D_nonseparable_2D_hu848ca1632001013b97e6e57dd07df9b3_17942_a78f25e2dee441fbce4cfa5ea1449262.webp"
               width="565"
               height="469"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Transformation über die feature map $\varphi$.
    </figcaption></figure>
</p>
<p>Dort sind die Daten linear separierbar, falls z.B.
$$\tilde{G} = \lbrace (-1,1), (-2,4) \rbrace \text{ and } \tilde{B} = \lbrace (-3,9), (1,1), (2,4)\rbrace,$$
und $\tilde{f}(x,y) = y+2.5x$, dann gilt $\tilde{G} = \lbrace (x,y) = \varphi(x) : x \in X \text{ and } \tilde{f}(x,y) &lt; 0\rbrace$, while $\tilde{B} = \lbrace (x,y) = \varphi(x) : x \in X \text{ and } \tilde{f}(x,y) \geq 0\rbrace$.</p>
<p>Wir bemerken hier noch, dass wir $f$ sich auch als Linearkombination der Kernfunktionen $K(-3,\cdot), K(-2,\cdot), K(-1,\cdot), K(1,\cdot), K(2,\cdot)$, ausgewertet an den $x$-Werten der Trainingsdaten darstellen können:
$$f(x) = \frac{7}{4} K(1,x) - \frac{3}{4}K(-1,x).$$
Das ist kein bloßer Zufall und wird gleich etwas algemeiner beschrieben.</p>
<p>An dieser Stelle möchte ich auch noch einmal betonen, dass der Satz von Moore-Aronszajn auf keinen Fall trivial ist und ein Verständnis seines Beweises eine solide Basis in Funkionalanalysis erfordert.</p>
<p>Zurück zu unserem Regressionsproblem und dem RKHS $H$. Ein allgemein großes Problem besteht im Overfitting der Trainingsdaten, was passieren kann, wenn wir ein Modell aufstellen wollen, das sehr komplex ist relativ zu den eigentlichen Daten, welches dann zu sensitiv auf Rauschen reagiert.<br>
In unserer Situation eines RKHS kann dies durch <em>Tikhonov Regularisierung</em> unterdrückt werden. Das bedeutet, wir fügen unserer Verlustfunktionen einen Regularisierungsterm hinzu, sodass
$$ L(f) = \frac{1}{n}\sum_{i=1}^n V(f(x_i),y_i) + c |f|_H^2,$$
wobei $c &gt; 0$ ein Regularisierungsparameter und $|f|_H = \sqrt{\langle f,f\rangle_H}$ die Norm von $f$ ist, welche durch das Skalarprodukt auf $H$ gegeben ist. Das ist in der Tat eine Regularisierung in dem Sinne, dass jedes $f \in H$ Lipschitz-stetig mit Lipschitz-Konstanten $|f|_H$ ist:
\begin{align*}
|f(z_1) - f(z_2)| = |\langle f,K(z_1,z_2)-K(z_1,z_2)\rangle| \leq  |f|_H d(z_1,z_2),
\end{align*}
wobei $d(z_1,z_2) := |K(z_1,\cdot)-K(z_2,\cdot)|_H$ als Distanz zwischen $z_1$ und $z_2$ aufgefasst werden kann. Diese Lipschitzstetigkeit bedeutet, dass die Steigung von $f$ beschränkt ist durch $|f|_H$. Daher, durch Hinzufügen dieses Terms zu unserer Verlustfunktion, welche wir minimieren wollen, wir der Overfitting-Problem entgegen wirken. Die folgende Skizze illustriert dies. Die Beziehung zwischen einem Input $x$ und dem label $y$ wirkt linear (evtl. mit leichter Krümmung). Allerdings trifft die orangene Kurve die Datenpunkte perfekt, während die Verlustfunktion für die lineare Kurve in grün positiv wäre.<br>
Dabei ist allerdings zu beachten, dass die orangene Kurve eine recht große Steigung an gewissen Stellen aufweist, um alle Punkte perfekt zu treffen. Durch Hinzufügen des Regularisierungsterms zur Verlustfunktion, wird diese aber auch wie gewünscht groß für die orangene Kurve, wodurch das lineare Modell eventuell mehr evorzugt wird.</p>
<p>















<figure  id="figure-overfitting-der-trainingsdaten">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="image" srcset="
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_8af8154c84a1d4eab27cf9382d93654e.webp 400w,
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_c0820a743c6aad0e69a7e504e80b2ae5.webp 760w,
               /de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_1200x1200_fit_q75_h2_lanczos.webp 1200w"
               src="/de/academia/ml/overfitting_hu980db8537e87a0a5158811606edfe1ce_25917_8af8154c84a1d4eab27cf9382d93654e.webp"
               width="760"
               height="394"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Overfitting der Trainingsdaten
    </figcaption></figure>
</p>
<p>Im allgemeinen kann es sehr schwierig bis hin zu unmöglich sein, den RKHS zu einem Kern $K$ zu bestimmen. Daher, selbst wenn sich das Minimierungsproblem in dem RKHS $H$ elegant lösen lässt , die Frage berechtigt, was der praktische Nutzen davon ist, wenn $H$ nicht explizit bekannt ist. Hier hilft erneu die abstrakte Theorie der RKHS, welche mittels des <a href="https://en.wikipedia.org/wiki/Representer_theorem" target="_blank" rel="noopener">Representer Theorems</a> folgendes aussagt:</p>
<p>In der gegebenen Situation mit Trainingsdaten $(x_i,y_i), i = 1,&hellip;,n$ ist die optimale Lösung (sofern sie existiert), gegeben durch
$$ f^*(x) = \sum_{i=1}^n \lambda_i K(x_i,x) $$
für gewisse Koeffizienten $\lambda_1,&hellip;,\lambda_n \in \mathbb{R}^n$.</p>
<p>Dadurch müssen wir nach dem Minimierer also nur in der Menge der Linearkombination der Funktionen $x \mapsto K(x_1,x),&hellip;,K(x_n,x)$ suchen, was essentiell lediglich lineare Regression in $\mathbb{R}^n$ ist.</p>
<p>Um dies Zusammenzufassen werfen wir einen Blick auf die nachfolgende Abbildung. Das untenstehende Diagramm kommutiert, das bedeutet, dass wir um einen Minimierer des Regressionsproblems zu finden, anstelle des Weges über die abstrakte Theorie direkt lineare Regression in  $\mathbb{R}^n$ durchführen können um optimale Koeffizienten $\lambda_1, &hellip; ,\lambda_n \in \mathbb{R}$ bezüglich der Verlustfunktion für die Funktion $f^*(x) = \sum_{i=1}^n \lambda_i K(x_i,x)$ zu bestimmen. Man nennt dieses Verfahren oft Ridge Regression.</p>
<p>















<figure  id="figure-suche-nach-optimalen-regressionsfunktionen-mittels-rkhs">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="image" srcset="
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_838e7e14f963fbe49e6f88532f02678d.webp 400w,
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_3da1440d2f29948fd8eafcaf148f1989.webp 760w,
               /de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_1200x1200_fit_q75_h2_lanczos.webp 1200w"
               src="/de/academia/ml/diagram_RKHS_hu98e4f96f463abae02a7b45daec58fb7a_62945_838e7e14f963fbe49e6f88532f02678d.webp"
               width="760"
               height="287"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Suche nach optimalen Regressionsfunktionen mittels RKHS
    </figcaption></figure>
</p>
<p>Im folgenden habe ich ein kleines Beispiel für Ridge Regression angefügt in der Form eines Jupyter Notebook, welches illustriert, dass sich die Methode eignen kann, um nicht-lineare Zusammenhänge zu erkennen ohne zu sensitiv auf Rauschen zu reagieren. Dafür habe ich die bereits implementierten Methoden in scikit-learn verwendet.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate the independent variable (x) as a random sample from a uniform distribution</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate the dependent variable (y) as sin(x) with some gaussian noise</span>
</span></span><span class="line"><span class="cl"><span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="n">noise</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot sample data</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nonlinear sample data&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>   
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_35af086aceddd76e13bb89981be66b2f.webp 400w,
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_50a1299c6152702312cacbb8d9a93427.webp 760w,
               /de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/de/academia/ml/ridge_reg_files/ridge_reg_0_0_hu925f354b133acfb4fa25a5003f8c7aa5_18051_35af086aceddd76e13bb89981be66b2f.webp"
               width="579"
               height="453"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Fit a ridge regression model with gaussian kernel</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Use grid-search cross-validation to find good parameter combinations alpha (regularization) and gamma = 1/sigma</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">kr_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">KernelRidge</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&#34;rbf&#34;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>    
</span></span><span class="line"><span class="cl">    <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">],</span> <span class="s2">&#34;gamma&#34;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">kr_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;training data&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">kr_cv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;y = predicted_labels(x)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;y = sin(x) (&#39;&#39;true&#39;&#39; labels)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;X&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Kernel ridge regression&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>&lt;matplotlib.legend.Legend at 0x238f07d75e0&gt;
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1344193a9fd31e6e1eab98c22c93a026.webp 400w,
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_4940ed2e2987ae24062590225aecb396.webp 760w,
               /de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/de/academia/ml/ridge_reg_files/ridge_reg_1_1_hub1a234681851d62ec836f4207e7cd568_39849_1344193a9fd31e6e1eab98c22c93a026.webp"
               width="579"
               height="453"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>Wie wir sehen, konnte unser Regressionsmodell in diesem Beispiel den nicht-linearen Zusammenhang zwischen $x$ und $y$ erkennen, ohne zu sehr durch die Störung bzw. das Rauschen beeinflusst zu werden.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/de/tag/maschinelles-lernen/">Maschinelles Lernen</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://raphael-wagner.netlify.app/"><img class="avatar mr-3 avatar-circle" src="/de/authors/admin/avatar_hueead46205df7a1e410342e864f7e1b3b_1078772_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://raphael-wagner.netlify.app/"></a></h5>
      <h6 class="card-subtitle">Doktorand und akademischer Angestellter</h6>
      <p class="card-text">Meine Forschungsinteressen betreffen schwache Lösungskonzepte in der mathematischen Strömungsmechanik. Insbesondere statistische und maßwertige Lösungen.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:raphael.wagner@mail.de" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/raphael-wagner-45a4212a0" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/de/themes/starter-hugo-academic/static/uploads/resume_wagner.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Raphael Wagner. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/de/js/wowchemy.min.4116c1dc326fa159b966c038aaa3c6ef.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Zitieren</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Kopie
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>


















</body>
</html>
